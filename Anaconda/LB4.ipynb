{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea7a54c2-c8a0-485b-a38d-2c82c6d1f85a",
   "metadata": {},
   "source": [
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "But this Model has size 15gb memory in disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdc16ed-edfa-422a-9a08-a47ffe0ddf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ollama\n",
    "!ollama pull llama3.2\n",
    "!ollama pull llama3.2-vision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37901ee8-ae3f-437d-ad18-972fda76b74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create file chat.py\n",
    "from ollama import chat\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Why is the sky blue?',\n",
    "    },\n",
    "]\n",
    "\n",
    "response = chat('llama3.2', messages=messages)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246a12be-5542-4bb5-b2c2-434288da24a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create file generate.py\n",
    "from ollama import generate\n",
    "prompt = \"Once upon a time in a land far, far away,\"\n",
    "response = generate('llama3.2', prompt=prompt)\n",
    "print(response['text'])\n",
    "python chat.py\n",
    "python generate.py"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd87dd68-5e48-43a5-a776-2d9d1b2d35b6",
   "metadata": {},
   "source": [
    "Using a visual model for image analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac19a45-0a46-4ec8-ab8d-153aec8cd895",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To use the llama3.2-vision model, we will create the file vision_chat.py\n",
    "import ollama\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='llama3.2-vision',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': 'What is in this image?',\n",
    "        'images': ['image.jpg']  #Path to JPG\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a09683-1b7b-41da-8cd8-46e6c6e2a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_with_history.py\n",
    "from ollama import chat\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Why is the sky blue?',\n",
    "    },\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': \"The sky is blue because of the way the Earth's atmosphere scatters sunlight.\",\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'What is the weather in Tokyo?',\n",
    "    },\n",
    "]\n",
    "\n",
    "while True:\n",
    "    user_input = input('Chat with history: ')\n",
    "    response = chat(\n",
    "        'llama3.2',\n",
    "        messages=messages + [{'role': 'user', 'content': user_input}]\n",
    "    )\n",
    "\n",
    "    messages += [\n",
    "        {'role': 'user', 'content': user_input},\n",
    "        {'role': 'assistant', 'content': response['message']['content']},\n",
    "    ]\n",
    "    print(response['message']['content'] + '\\n')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da81d59e-ca4b-4f98-96bb-c5efcc34e5ea",
   "metadata": {},
   "source": [
    "Generate - Generate text with a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7193ce20-128a-4304-80bd-0320da2de7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import generate\n",
    "response = generate('llama3.2', 'Why is the sky blue?')\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4220e5-4426-4268-8dd5-ecfaaee225b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import ollama\n",
    "\n",
    "\n",
    "async def main():\n",
    "  client = ollama.AsyncClient()\n",
    "  response = await client.generate('llama3.2', 'Why is the sky blue?')\n",
    "  print(response['response'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  try:\n",
    "    asyncio.run(main())\n",
    "  except KeyboardInterrupt:\n",
    "    print('\\nGoodbye!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28536ccd-6e4a-4fe8-9aba-f0b4fd882a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import generate\n",
    "for part in generate('llama3.2', 'Why is the sky blue?', stream=True):\n",
    "  print(part['response'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70901dde-4655-4099-a979-62b4618f4c51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
